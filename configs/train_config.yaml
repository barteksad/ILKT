# train_config.yaml

defaults:
  - _self_
  - model: starter
  - datasets:
    - contrastive/sentence-compression/sentence-compression.yaml
    - mlm/wikipedia/wikipedia.yaml

exp:
  log_dir: # set during runtime to automatically created dir
  pretrained_model_name_or_path: "microsoft/mdeberta-v3-base"
  max_length: 128
  batch_size: 4
  seed: 42
  training_steps: 10
  optimizer:
    opt: "adamw"
    lr: 5e-5
    weight_decay: 0.0
    filter_bias_and_bn: true

fabric:
  _target_: lightning.Fabric
  num_nodes: 1
  devices: 1

wandb:
  project: "inter-lingual_knowledge_transferring_in_NLP_embeddings"
