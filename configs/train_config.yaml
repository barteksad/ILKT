# train_config.yaml

defaults:
  - _self_
  - model: starter
  - train_datasets:
    - contrastive/all-nli/all-nli-pair-score.yaml
    - contrastive/eli5/eli5.yaml
    - contrastive/coliee/coliee.yaml
    - contrastive/pubmedqa/pubmedqa.yaml
    - contrastive/hotpotqa/hotpotqa.yaml
    - contrastive/gooaq/gooaq.yaml
    - contrastive/msmarco-distilbert-margin-mse-sym-mnrl-mean-v1/msmarco-distilbert-margin-mse-sym-mnrl-mean-v1.yaml
    - contrastive/msmarco-msmarco-distilbert-base-tas-b/msmarco-msmarco-distilbert-base-tas-b-triplet.yaml
    - contrastive/natural-questions/natural-questions.yaml
    - contrastive/quora-duplicates/quora-duplicates.yaml
    - contrastive/squad/squad.yaml
    - contrastive/stsb/stsb.yaml
    - contrastive/yahoo-answers/yahoo-answers.yaml
    - contrastive/eli5/eli5.yaml
    - sentence_classification/klej-cdsc-e/klej-cdsc-e.yaml
    - sentence_classification/klej-psc/klej-psc.yaml
    - sentence_classification/klej-dyk/klej-dyk.yaml
    - sentence_classification/scifield/scifield.yaml
    - mlm/oscar/oscar.yaml
    - mlm/allegro-summarization-articles/allegro-summarization-articles.yaml
    - mlm/polemo2-official/polemo2-official.yaml
    - mlm/polish-news/polish-news.yaml
    # - mlm/wikipedia-pl/wikipedia-pl.yaml
   

  - val_datasets:
    - contrastive/all-nli/all-nli-pair-score.yaml
    - sentence_classification/klej-cdsc-e/klej-cdsc-e_valid.yaml
    - sentence_classification/klej-psc/klej-psc_valid.yaml
    - sentence_classification/klej-dyk/klej-dyk_valid.yaml
    - sentence_classification/scifield/scifield.yaml
    - mlm/allegro-summarization-articles/allegro-summarization-articles.yaml
    # - sentence_classification/wikinews-pl-clustering-p2p/wikinews-pl-clustering-p2p.yaml

exp:
  log_dir: # set during runtime to automatically created dir
  pretrained_model_name_or_path: google-bert/bert-base-multilingual-cased
  max_length: 128
  contrastive_batch_size: 16
  batch_size: 8
  valid_batch_size: 8
  seed: 42
  beta: 1
  training_steps: 1_000_000
  validate_every: 30_000
  n_examples_per_valid_dataset: 20_000
  save_every: 100_000
  max_grad_norm: 1.0
  monitor_stiffness_every: 10_000
  monitor_stiffness_steps: 100
  optimizer:
    # opt: "adamw"
    opt: "bnbadamw8bit"
    lr: 5e-5
    weight_decay: 0.0
    filter_bias_and_bn: true
    # fused: true
  scheduler: # must be one of those: https://huggingface.co/docs/transformers/en/main_classes/optimizer_schedules#schedules
    _target_: transformers.get_cosine_schedule_with_warmup
    num_warmup_steps: 1000
    num_training_steps: ${exp.training_steps}
    num_cycles: 0.5

fabric:
  _target_: lightning.Fabric
  num_nodes: 1
  devices: 1

wandb:
  project: "inter-lingual_knowledge_transferring_in_NLP_embeddings"
  entity: "top-notch-nlp"
