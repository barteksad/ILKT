# train_config.yaml

defaults:
  - _self_
  - model: starter
  - train_datasets:
    - contrastive/all-nli/all-nli-pair-score.yaml
    - contrastive/gooaq/gooaq.yaml
    - contrastive/msmarco-distilbert-margin-mse-sym-mnrl-mean-v1/msmarco-distilbert-margin-mse-sym-mnrl-mean-v1.yaml
    - contrastive/msmarco-msmarco-distilbert-base-tas-b/msmarco-msmarco-distilbert-base-tas-b-triplet.yaml
    - contrastive/natural-questions/natural-questions.yaml
    - contrastive/quora-duplicates/quora-duplicates.yaml
    - contrastive/squad/squad.yaml
    - contrastive/stsb/stsb.yaml
    - contrastive/yahoo-answers/yahoo-answers.yaml
  - val_datasets:
    - contrastive/all-nli/all-nli-pair-score.yaml

exp:
  log_dir: # set during runtime to automatically created dir
  pretrained_model_name_or_path: microsoft/mdeberta-v3-base
  max_length: 128
  batch_size: 4
  seed: 42
  training_steps: 20_000
  validate_every: 1000
  n_examples_per_valid_dataset: 1000
  save_every: 1000
  optimizer:
    opt: "adam"
    lr: 2e-5
    weight_decay: 0.0
    filter_bias_and_bn: true

fabric:
  _target_: lightning.Fabric
  num_nodes: 1
  devices: 1

wandb:
  project: "inter-lingual_knowledge_transferring_in_NLP_embeddings"
  entity: "top-notch-nlp"
